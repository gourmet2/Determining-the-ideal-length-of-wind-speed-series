{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "790390ac",
   "metadata": {},
   "source": [
    "# 0: Import modules and read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5503d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import random\n",
    "from scipy.stats import skew, kurtosis, weibull_min\n",
    "from scipy.special import gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872a0dff",
   "metadata": {},
   "source": [
    "### 0.1 Import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c597291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------- Function 1 ----------------------------\n",
    "def acquire_wsp_ds(file_path, start=\"1979-01-01\", end=\"2024-12-31\"):\n",
    "    ds = xr.open_dataset(file_path)\n",
    "    s = (\n",
    "        ds[\"windspeeds\"]\n",
    "        .sel(time=slice(start, end))\n",
    "        .to_series()                     \n",
    "        .replace(-2e30, np.nan)         \n",
    "        .sort_index()\n",
    "    )\n",
    "    return s\n",
    "\n",
    "# ---------------------------- Function 2 ----------------------------\n",
    "def weibull_params_and_wpd(data, air_density=1.225):\n",
    "    data = data.dropna()\n",
    "    if len(data) < 10:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    else:\n",
    "        shape_p, _, scale_p = weibull_min.fit(data, floc=0, method=\"MLE\")\n",
    "        wpd = 0.5 * air_density * (scale_p ** 3) * gamma(1 + 3 / shape_p)\n",
    "        return shape_p, scale_p, wpd\n",
    "\n",
    "# ---------------------------- Function 3 ----------------------------\n",
    "def statistic_value(dataset):\n",
    "    statis = []\n",
    "    \n",
    "    statis.append(np.mean(dataset))\n",
    "    statis.append(np.std(dataset,axis=0)[0])\n",
    "   \n",
    "    statis.append(skew(dataset)[0])\n",
    "\n",
    "    statis.append(kurtosis(dataset)[0])\n",
    "    \n",
    "    shape_p, _, scale_p= weibull_min.fit(dataset, floc=0, method=\"MLE\")\n",
    "    w_wpd = 0.5 * 1.225 * (scale_p ** 3) * gamma(1 + 3 / shape_p) # wind power density\n",
    "    \n",
    "    statis.append(shape_p)\n",
    "    statis.append(scale_p)\n",
    "    statis.append(w_wpd)\n",
    "    \n",
    "    return statis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aa19b8",
   "metadata": {},
   "source": [
    "### 0.2 Acquire data information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b3388c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>windspeeds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1979-01-01 00:00:00</td>\n",
       "      <td>4.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1979-01-01 01:00:00</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1979-01-01 02:00:00</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1979-01-01 03:00:00</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1979-01-01 04:00:00</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403243</th>\n",
       "      <td>2024-12-31 19:00:00</td>\n",
       "      <td>12.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403244</th>\n",
       "      <td>2024-12-31 20:00:00</td>\n",
       "      <td>14.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403245</th>\n",
       "      <td>2024-12-31 21:00:00</td>\n",
       "      <td>12.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403246</th>\n",
       "      <td>2024-12-31 22:00:00</td>\n",
       "      <td>12.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403247</th>\n",
       "      <td>2024-12-31 23:00:00</td>\n",
       "      <td>12.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>403248 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      time  windspeeds\n",
       "0      1979-01-01 00:00:00         4.6\n",
       "1      1979-01-01 01:00:00         4.1\n",
       "2      1979-01-01 02:00:00         3.6\n",
       "3      1979-01-01 03:00:00         3.6\n",
       "4      1979-01-01 04:00:00         3.1\n",
       "...                    ...         ...\n",
       "403243 2024-12-31 19:00:00        12.4\n",
       "403244 2024-12-31 20:00:00        14.4\n",
       "403245 2024-12-31 21:00:00        12.9\n",
       "403246 2024-12-31 22:00:00        12.9\n",
       "403247 2024-12-31 23:00:00        12.9\n",
       "\n",
       "[403248 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#station = '031710-99999'\n",
    "file_path = \"hadisd.3.4.2.202501p_19310101-20250201_031710-99999.nc\"\n",
    "wsp = acquire_wsp_ds(file_path)\n",
    "\n",
    "# Interpolate to hourly data\n",
    "wsp = wsp.asfreq(\"h\").interpolate(method=\"time\")\n",
    "df_ip = wsp.rename_axis(\"time\").to_frame(\"windspeeds\").reset_index()\n",
    "\n",
    "# The length of df_ip is 403248 hours from 1979-01-01 to 2024-12-31\n",
    "df_ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdb5f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_df(df):\n",
    "    \n",
    "    time_df = pd.DataFrame({\n",
    "        'Year': pd.to_datetime(df['time'], format=\"%Y-%m-%dT%H:%M:%S.%fZ\").dt.year,\n",
    "        'Month': pd.to_datetime(df['time'], format=\"%Y-%m-%dT%H:%M:%S.%fZ\").dt.month,\n",
    "        'Day': pd.to_datetime(df['time'], format=\"%Y-%m-%dT%H:%M:%S.%fZ\").dt.day,\n",
    "        'Hour': pd.to_datetime(df['time'], format=\"%Y-%m-%dT%H:%M:%S.%fZ\").dt.hour,\n",
    "        'Minute': pd.to_datetime(df['time'], format=\"%Y-%m-%dT%H:%M:%S.%fZ\").dt.minute,\n",
    "        'Second': pd.to_datetime(df['time'], format=\"%Y-%m-%dT%H:%M:%S.%fZ\").dt.second,\n",
    "    })\n",
    "    \n",
    "    return time_df\n",
    "\n",
    "time_df_new_merged_series = get_time_df(df_ip)\n",
    "new_merged_series_value = df_ip['windspeeds'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d87fa4",
   "metadata": {},
   "source": [
    "# 1: Strategies for sampling experiments\n",
    "\n",
    "->> Run this section only if you need samples from the experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fc1daf",
   "metadata": {},
   "source": [
    "## 1.1: Strategy 1-Continous sampling\n",
    "\n",
    "->> This section can be run without running the other strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e517a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The estimated time for this script to run is approximately 12 hours.\n",
    "\n",
    "data = df_ip.dropna() \n",
    "\n",
    "output_dir = '031710-99999_parameters_under_sequential_experiment' # {output directory} - need to be adjusted by your own settings\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "yrs_in_total = 12 # {the total years of data} - need to be adjusted by the length of your own data\n",
    "windows = [720 * i for i in range(1, 12 * yrs_in_total + 1)]\n",
    "\n",
    "for w in windows:\n",
    "    start_time = time.time()\n",
    "    file_path = os.path.join(output_dir, f'df_stats_window_{w//720}_months.csv')\n",
    "\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        \n",
    "        max_start = len(data) - w\n",
    "        if max_start < 1000:\n",
    "            # If the realization candidates < 1000, calculate all windows\n",
    "            sample_indices = range(w - 1, len(data))\n",
    "        else:\n",
    "            # If the realizations candidates > 1000, randomly sample 1000 window end indices, ensuring fixed sampling (due to np.random.seed)\n",
    "            np.random.seed(int(w / 720))\n",
    "            sample_indices = np.random.choice(range(w - 1, len(data)), size=1000, replace=False)\n",
    "            sample_indices = np.sort(sample_indices)  \n",
    "            \n",
    "        means = []\n",
    "        stds = []\n",
    "        skews = []\n",
    "        kurtoses = []\n",
    "        weibull_shapes = []\n",
    "        weibull_scales = []\n",
    "        weibull_wpds = []\n",
    "\n",
    "        for i in sample_indices:\n",
    "            window_data = data.iloc[i - w + 1:i + 1]\n",
    "            means.append(window_data.mean().iloc[0])\n",
    "            stds.append(window_data.std().iloc[0])\n",
    "            skews.append(skew(window_data, bias=False)[0])\n",
    "            kurtoses.append(kurtosis(window_data, bias=False)[0])\n",
    "            shape, scale, wpd = weibull_params_and_wpd(window_data)\n",
    "            weibull_shapes.append(shape)\n",
    "            weibull_scales.append(scale)\n",
    "            weibull_wpds.append(wpd)\n",
    "\n",
    "        df_stats = pd.DataFrame({\n",
    "            'mean': means,\n",
    "            'std': stds,\n",
    "            'skew': skews,\n",
    "            'kurtosis': kurtoses,\n",
    "            'weibull_shape': weibull_shapes,\n",
    "            'weibull_scale': weibull_scales,\n",
    "            'weibull_power_density': weibull_wpds\n",
    "        })\n",
    "        df_stats.to_csv(file_path, index=False)\n",
    "    end_time = time.time()\n",
    "    print(f\"Window {w//720} (unit: months) finished calculating; consuming time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c88b10",
   "metadata": {},
   "source": [
    "## 1.2 Strategy 2-Random sampling\n",
    "\n",
    "->> This section can be run without running the other strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f9bea9",
   "metadata": {},
   "source": [
    "#### 2.1 When the gap is 1 month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99518fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = df_ip.dropna() \n",
    "\n",
    "output_dir = '031710-99999_parameters_under_random_experiment' # {output directory} - need to be adjusted by your own settings\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "yrs_in_total = 12\n",
    "windows = [720 * i for i in range(1, 12 * yrs_in_total + 1)]\n",
    "data = df_ip.dropna()\n",
    "\n",
    "iteration_time = 1000\n",
    "\n",
    "for w in windows:\n",
    "    start_time = time.time()\n",
    "    file_path = os.path.join(output_dir, f'df_stats_window_random_{w//720}_months.csv')\n",
    "    if not os.path.exists(file_path):\n",
    "        \n",
    "        rng = np.random.default_rng(seed=w) # seed set to window size for reproducibility\n",
    "        random_numbers = rng.integers(low=0, high=len(data), size=(iteration_time, w))\n",
    "        \n",
    "        stat_values  = np.empty((iteration_time, 7))\n",
    "    \n",
    "        for j in range(iteration_time):\n",
    "            stat_values[j,:] = statistic_value(data.iloc[random_numbers[j,:]].values)\n",
    "            \n",
    "        df = pd.DataFrame(stat_values, columns=['mean','std','skew','kurtosis','weibull_shape',\n",
    "                 'weibull_scale','weibull_power_density'])\n",
    "        df.to_csv(file_path, index=False)\n",
    "        \n",
    "    end_time = time.time()\n",
    "    print(f\"Window {w//720} (unit: months) finished calculating; consuming time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039d97d0",
   "metadata": {},
   "source": [
    "#### 2.2 When the gap is 10 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e201ef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "interval = 24*10 \n",
    "start_range = 24*30  \n",
    "end_range = 24*365*6  \n",
    "\n",
    "\n",
    "iteration_time = 1000\n",
    "num_values = int((end_range-start_range)/interval + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5783f13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_merged_series_value = df_ip['windspeeds'].values\n",
    "\n",
    "n = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(start_range, end_range+1, interval):   \n",
    "    \n",
    "    n += 1\n",
    "    \n",
    "    print(f\"No.{(n-1),num_values} file is being processed\")\n",
    "          \n",
    "    if os.path.exists('../result_iteration_random_{}.csv'.format(n-1)):\n",
    "        continue\n",
    "    \n",
    "    rng = np.random.default_rng(seed=i) \n",
    "    \n",
    "    random_numbers = rng.integers(low=0, high=len(new_merged_series_value), size=(iteration_time, i))\n",
    "    \n",
    "    stat_values  = np.empty((iteration_time, 7))\n",
    "    \n",
    "    for j in range(iteration_time):\n",
    "        stat_values[j,:] = statistic_value(new_merged_series_value[random_numbers[j,:]])\n",
    "    \n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame(stat_values, columns=['Mean','Std','Skewness','Kurtosis','Shape','Scale','Energy density'])\n",
    "    df.to_csv('../Data/New_stations_data_density_16yr_random/SN50500/result_iteration_{}.csv'.format(n-1), index=False)  # 保存到 CSV 文件\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(\"Execution_time: \", execution_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1310953d",
   "metadata": {},
   "source": [
    "## 1.3 Strategy 3-Diurnal-retained sampling\n",
    "\n",
    "->> This section can be run without running the other strategies.\n",
    "\n",
    "->> Samples are equally selected from four time periods (0-5, 6-11, 12-17, 18-23)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c064e814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100812 100812 100812 100812\n",
      "0.0 0.0 0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Acquire indices of four time periods: 0-5, 6-11, 12-17, 18-23\n",
    "loc_05 = time_df_new_merged_series.index[(time_df_new_merged_series['Hour'] >= 0) & (time_df_new_merged_series['Hour'] <= 5)].tolist()\n",
    "loc_611 = time_df_new_merged_series.index[(time_df_new_merged_series['Hour'] >= 6) & (time_df_new_merged_series['Hour'] <= 11)].tolist()\n",
    "loc_1217 = time_df_new_merged_series.index[(time_df_new_merged_series['Hour'] >= 12) & (time_df_new_merged_series['Hour'] <= 17)].tolist()\n",
    "loc_1823 = time_df_new_merged_series.index[(time_df_new_merged_series['Hour'] >= 18) & (time_df_new_merged_series['Hour'] <= 23)].tolist()\n",
    "\n",
    "# Print the lengths of these indices to see whether the four time periods have similar data amounts\n",
    "print(len(loc_05), len(loc_611), len(loc_1217), len(loc_1823))\n",
    "mean_loc = (len(loc_05)+ len(loc_611)+ len(loc_1217)+ len(loc_1823))/4\n",
    "\n",
    "print(len(loc_05)-mean_loc, len(loc_611)-mean_loc, len(loc_1217)-mean_loc, len(loc_1823)-mean_loc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a8b915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "\n",
    "interval = 24*10 # The interval is 10 days (10*24 hours)\n",
    "start_range = 24*30 # Start from 1 month\n",
    "end_range = 24*365*6  # covering 6 years    \n",
    "\n",
    "iteration_time = 1000\n",
    "num_values = int((end_range-start_range)/interval + 1)\n",
    "\n",
    "print(num_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec45404",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_merged_series_value = df_ip['windspeeds'].values\n",
    "\n",
    "n = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(start_range, end_range+1, interval):\n",
    "    \n",
    "    n += 1\n",
    "    \n",
    "    print(f\"No.{(n-1),num_values} file is being processed\")\n",
    "    \n",
    "    if os.path.exists('../result_iteration_diurnal_{}.csv'.format(n-1)):        \n",
    "        continue\n",
    "    \n",
    "    random.seed(i)\n",
    "    \n",
    "    stat_values  = np.empty((iteration_time, 7))\n",
    "    num_sel = int(i/4) # the number of data to be selected from each time period\n",
    "    \n",
    "    for j in range(iteration_time):\n",
    "        # select certain number of data from each of the four groups, once per iteration        \n",
    "        random_selection_05 = random.choices(loc_05, k=num_sel)\n",
    "        random_selection_611 = random.choices(loc_611, k=num_sel)\n",
    "        random_selection_1217 = random.choices(loc_1217, k=num_sel)\n",
    "        random_selection_1823 = random.choices(loc_1823, k=num_sel)\n",
    "\n",
    "        random_selection = [item for sublist in [random_selection_05, random_selection_611, random_selection_1217, random_selection_1823] for item in sublist]\n",
    "        \n",
    "        stat_values[j,:] = statistic_value(new_merged_series_value[random_selection])\n",
    "    \n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame(stat_values, columns=['Mean','Std','Skewness','Kurtosis','Shape','Scale','Energy density'])\n",
    "    df.to_csv('../031710-99999_parameters_under_diurnalcycle_experiment/result_iteration_diurnal_{}.csv'.format(n-1), index=False)  \n",
    "\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(\"Execution_time: \", execution_time, \"seconds\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a18918",
   "metadata": {},
   "source": [
    "## 1.4: Strategy 4-Seasonality-retained sampling\n",
    "\n",
    "->> This section can be run without running the other strategies.\n",
    "\n",
    "->> Samples are equally selected from each month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191003b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acquire indices of 12 months\n",
    "loc_month = {}\n",
    "num_month = []\n",
    "for i in range(1,13):\n",
    "    loc_month[str(i)] = time_df_new_merged_series.index[time_df_new_merged_series['Month'] == i].tolist()\n",
    "    num_month.append(len(time_df_new_merged_series.index[time_df_new_merged_series['Month'] == i].tolist()))\n",
    "\n",
    "print(num_month-np.mean(num_month))\n",
    "\n",
    "\n",
    "# plot the number differences between each month and the mean value\n",
    "plt.plot(num_month-np.mean(num_month))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e5130e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "\n",
    "interval = 24*10 \n",
    "start_range = 24*30 \n",
    "end_range = 24*365*6  \n",
    "\n",
    "iteration_time = 1000\n",
    "num_values = int((end_range-start_range)/interval + 1)\n",
    "\n",
    "print(num_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886bf1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_merged_series_value = df_ip['windspeeds'].values\n",
    "\n",
    "n = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(start_range, end_range+1, interval):\n",
    "    \n",
    "    n += 1\n",
    "    \n",
    "    print(f\"No.{(n-1),num_values} file is being processed\")\n",
    "    \n",
    "    if os.path.exists('../result_iteration_seasonality_{}.csv'.format(n-1)):        \n",
    "        continue\n",
    "    \n",
    "    random.seed(i)\n",
    "\n",
    "    \n",
    "    stat_values  = np.empty((iteration_time, 7))\n",
    "    num_sel = int(i/12) \n",
    "    \n",
    "    for j in range(iteration_time):\n",
    "        \n",
    "        \n",
    "        random_selection_1 = random.choices(loc_month['1'], k=num_sel)\n",
    "        random_selection_2 = random.choices(loc_month['2'], k=num_sel)\n",
    "        random_selection_3 = random.choices(loc_month['3'], k=num_sel)\n",
    "        random_selection_4 = random.choices(loc_month['4'], k=num_sel)\n",
    "        random_selection_5 = random.choices(loc_month['5'], k=num_sel)\n",
    "        random_selection_6 = random.choices(loc_month['6'], k=num_sel)\n",
    "        random_selection_7 = random.choices(loc_month['7'], k=num_sel)\n",
    "        random_selection_8 = random.choices(loc_month['8'], k=num_sel)\n",
    "        random_selection_9 = random.choices(loc_month['9'], k=num_sel)\n",
    "        random_selection_10 = random.choices(loc_month['10'], k=num_sel)\n",
    "        random_selection_11 = random.choices(loc_month['11'], k=num_sel)\n",
    "        random_selection_12 = random.choices(loc_month['12'], k=num_sel)\n",
    "\n",
    "        random_selection = [item for sublist in [random_selection_1, random_selection_2, random_selection_3, \n",
    "                                                 random_selection_4, random_selection_5, random_selection_6,\n",
    "                                                 random_selection_7, random_selection_8, random_selection_9,\n",
    "                                                 random_selection_10, random_selection_11, random_selection_12] for item in sublist]\n",
    "        \n",
    "        stat_values[j,:] = statistic_value(new_merged_series_value[random_selection])\n",
    "    \n",
    "\n",
    "    df = pd.DataFrame(stat_values, columns=['Mean','Std','Skewness','Kurtosis','Shape','Scale','Energy density'])\n",
    "    df.to_csv('../031710-99999_parameters_under_seasonality_experiment/result_iteration_seasonality_{}.csv'.format(n-1), index=False) \n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(\"Execution_time: \", execution_time, \"seconds\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0416b6bf",
   "metadata": {},
   "source": [
    "# 2: Analyzing and visualizing the results from sampling experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d9bbd2",
   "metadata": {},
   "source": [
    "### 2.0 import functions for this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bd0cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------- Function 1 ----------------------------\n",
    "# Function 1： define the function to read data\n",
    "def get_exp_data(folder_path, file_name_start, num_of_files, start_range, interval):\n",
    "    \n",
    "    table_prob = np.full([1000,num_of_files,7],np.nan) # \n",
    "    x_scatter = np.full([1000,num_of_files],np.nan) # \n",
    "\n",
    "    prob_name = ['Mean','Std','Skewness','Kurtosis','Shape','Scale','Energy density']\n",
    "\n",
    "    for i in range(num_of_files):\n",
    "\n",
    "        x_scatter[:,i] = start_range + i*interval                 \n",
    "\n",
    "        file_name = file_name_start + '_' + str(i) + \".csv\"\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        for j in range(7):\n",
    "            table_prob[:,i,j] = df[prob_name[j]] \n",
    "    \n",
    "    return table_prob, x_scatter\n",
    "\n",
    "\n",
    "# ---------------------------- Function 2 ----------------------------\n",
    "# Function 2： define the function to read data\n",
    "def get_statisc_of_series(new_merged_series_value):\n",
    "\n",
    "    statisc_totalseries = np.empty([7,7])  # \n",
    "\n",
    "    statisc_totalseries[:,0] = statistic_value(new_merged_series_value)*1\n",
    "    statisc_totalseries[:,1] = statisc_totalseries[:,0]*(1+0.1) # +10%\n",
    "    statisc_totalseries[:,2] = statisc_totalseries[:,0]*(1-0.1) # -10%\n",
    "    statisc_totalseries[:,3] = statisc_totalseries[:,0]*(1+0.05) # +5%\n",
    "    statisc_totalseries[:,4] = statisc_totalseries[:,0]*(1-0.05) # -5%\n",
    "    statisc_totalseries[:,5] = statisc_totalseries[:,0]*(1+0.02) # +2%\n",
    "    statisc_totalseries[:,6] = statisc_totalseries[:,0]*(1-0.02) # -2%\n",
    "    \n",
    "    \n",
    "    return statisc_totalseries\n",
    "\n",
    "\n",
    "# ---------------------------- Function 3 ----------------------------\n",
    "# Function 3： define the function to read data\n",
    "def range_confidence_level(dataset, z_value): # 1.645-90%, 1.960-95%, 2.326-98%, 2.576-99%\n",
    "    mean_dataset = np.mean(dataset)\n",
    "    std_dataset = np.std(dataset)\n",
    "    \n",
    "    confidence_interval = [mean_dataset+z_value*(std_dataset), mean_dataset-z_value*(std_dataset)]\n",
    "    \n",
    "    ## Another method to calculate confidence interval\n",
    "    #confidence_interval = [mean_dataset+z_value*(std_dataset/np.sqrt(len(dataset))), mean_dataset-z_value*(std_dataset/np.sqrt(len(dataset)))]\n",
    "    \n",
    "    return confidence_interval\n",
    "\n",
    "\n",
    "# ---------------------------- Function 4 ----------------------------\n",
    "# Function 4： \n",
    "def get_confidence_level(table_prob):\n",
    "    value_confidence_level = np.full([np.shape(table_prob)[1], 8, np.shape(table_prob)[2]], np.nan) \n",
    "\n",
    "    for i in range(np.shape(table_prob)[1]): # 217 experiments in total\n",
    "        for j in range(np.shape(table_prob)[2]): # seven parameters in total\n",
    "\n",
    "            value_confidence_level[i, 0, j] = range_confidence_level(table_prob[:,i,j], 1.645)[0] # 求90%的区间\n",
    "            value_confidence_level[i, 1, j] = range_confidence_level(table_prob[:,i,j], 1.645)[1] # 求90%的区间\n",
    "\n",
    "            value_confidence_level[i, 2, j] = range_confidence_level(table_prob[:,i,j], 1.96)[0] # 求95%的区间\n",
    "            value_confidence_level[i, 3, j] = range_confidence_level(table_prob[:,i,j], 1.96)[1] # 求95%的区间\n",
    "\n",
    "            value_confidence_level[i, 4, j] = range_confidence_level(table_prob[:,i,j], 2.326)[0] # 求98%的区间\n",
    "            value_confidence_level[i, 5, j] = range_confidence_level(table_prob[:,i,j], 2.326)[1] # 求98%的区间\n",
    "\n",
    "            value_confidence_level[i, 6, j] = range_confidence_level(table_prob[:,i,j], 2.576)[0] # 求99%的区间\n",
    "            value_confidence_level[i, 7, j] = range_confidence_level(table_prob[:,i,j], 2.576)[1] # 求99%的区间\n",
    "            \n",
    "    return value_confidence_level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fdb885",
   "metadata": {},
   "source": [
    "### 2.1 read the data from the sampling experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df6f1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "interval = 24*10 \n",
    "start_range = 24*30  \n",
    "end_range = 24*365*6  \n",
    "\n",
    "iteration_time = 1000\n",
    "\n",
    "num_of_files = int((end_range-start_range)/interval + 1)\n",
    "\n",
    "# get the distribution parameters of the total series\n",
    "statisc_totalseries = get_statisc_of_series(new_merged_series_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c30bc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------- 1. Acquire data from random experiments ----------------------------\n",
    "folder_path = \"../Data/New_stations_data_density_16yr_random/SN50500\" # {file folder path} - need to be adjusted by your own settings\n",
    "\n",
    "# a. get the distribution parameters of all the experiments\n",
    "table_random, x_scatter = get_exp_data(folder_path,'result_iteration', num_of_files, start_range, interval)\n",
    "\n",
    "# b. get the 90%, 95%, 99% confidence level of each experiment\n",
    "confidence_level_random = get_confidence_level(table_random)\n",
    "\n",
    "\n",
    "# ---------------------------- 2.Acquire data from diurnal-cycle retained experiments ----------------------------\n",
    "\n",
    "folder_path = \"../Data/New_stations_data_density_16yr_withdiurnalcycle/SN50500\"\n",
    "table_diurnal, _ = get_exp_data(folder_path,'result_iteration', num_of_files, start_range, interval)\n",
    "confidence_level_diurnal = get_confidence_level(table_diurnal)\n",
    "\n",
    "# ---------------------------- 3. Acquire data from seasonality retained experiments ----------------------------\n",
    "\n",
    "folder_path = \"../Data/New_stations_data_density_16yr_withseasonality/12_months/SN50500\"\n",
    "table_seasonality, _ = get_exp_data(folder_path,'result_iteration', num_of_files, start_range, interval)\n",
    "confidence_level_seasonality = get_confidence_level(table_seasonality)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda1cd6d",
   "metadata": {},
   "source": [
    "#### 2.1.1 Plot figure 2 in the manuscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34c5ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(2.6, 7))\n",
    "prob_name = ['Mean','Std','Skewness','Kurtosis','Shape','Scale','Energy density']\n",
    "ylabel_name = ['Mean $(m\\ s^{-1})$', 'Std. dev. $(m\\ s^{-1})$','Skewness','Kurtosis','Weibull k','Weibull c $(m\\ s^{-1})$','Energy density $(W\\ m^{-2})$']\n",
    "ylim_value = {'Mean':[2.,4.],'Std':[2.,4.],'Skewness':[0.,4.],'Kurtosis':[-10,40],'Shape':[1.1,2.1],'Scale':[3.6,4.6],'Energy density':[50,120]}\n",
    "\n",
    "for i, ax in zip(range(3), axes.flatten()):\n",
    "    \n",
    "    \n",
    "    if ax.get_visible():  \n",
    "        \n",
    "        \n",
    "        if i == 0:\n",
    "            i_x = 0\n",
    "            ax.set_title('SN50500')\n",
    "            ax.set_ylabel(ylabel_name[i_x], fontsize=12)\n",
    "            \n",
    "        elif i==1:\n",
    "            i_x = 5\n",
    "            ax.set_ylabel(ylabel_name[i_x], fontsize=12)\n",
    "            \n",
    "        elif i==2:\n",
    "            i_x = 6\n",
    "            ax.set_ylabel('Power density $(W\\ m^{-2})$', fontsize=12)\n",
    "            ax.set_xlabel('Number of data')\n",
    "           \n",
    "\n",
    "        # setting x axis as log scale\n",
    "        ax.set_xscale('log') \n",
    "\n",
    "        # showing ±5% uncertainty bands\n",
    "        ax.axhspan(ymin=statisc_totalseries[i_x,4], ymax=statisc_totalseries[i_x,3], color=\"#EFF3FF\", zorder=0, label='±5% uncertainty') # 5% \n",
    "        ax.axhspan(ymin=statisc_totalseries[i_x,6], ymax=statisc_totalseries[i_x,5], color=\"#B0C4DE\", zorder=0, label='±2% uncertainty') # 2%\n",
    "\n",
    "        ax.set_ylim(ylim_value[prob_name[i_x]])\n",
    "        \n",
    "        for j in range(num_of_files+1):\n",
    "\n",
    "            if j == num_of_files:\n",
    "\n",
    "                ax.scatter(len(new_merged_series_value), statistic_value(new_merged_series_value)[i_x], marker='*', color='r', s=8, zorder=1)\n",
    "            else:\n",
    "                ax.scatter(x_scatter[:,j], table_random[:,j,i_x], marker='*', color='k',s=1, alpha=0.2, zorder=1)\n",
    "\n",
    "\n",
    "\n",
    "        # 展示90%的范围\n",
    "        ax.plot(x_scatter[0,:], confidence_level_random[:,0,i_x], color=\"#E69F00\", label='Random', linewidth=3, zorder=3)\n",
    "        ax.plot(x_scatter[0,:], confidence_level_random[:,1,i_x], color=\"#E69F00\", linewidth=3, zorder=3)\n",
    "        \n",
    "        ax.plot(x_scatter[0,:], confidence_level_diurnal[:,0,i_x], color=\"#A07EB3\", linestyle='--',label='Diurnal', linewidth=3, zorder=4)\n",
    "        ax.plot(x_scatter[0,:], confidence_level_diurnal[:,1,i_x], color=\"#A07EB3\", linestyle='--', linewidth=3, zorder=4)\n",
    "\n",
    "        ax.plot(x_scatter[0,:], confidence_level_seasonality[:,0,i_x], color=\"#4E79A7\", linestyle=':',label='Seasonal', linewidth=3, zorder=5)\n",
    "        ax.plot(x_scatter[0,:], confidence_level_seasonality[:,1,i_x], color=\"#4E79A7\", linestyle=':', linewidth=3, zorder=5)        \n",
    "        \n",
    "        ax.minorticks_on()\n",
    "        \n",
    "        #ax.text(-0.2, 1.1, labels[i], transform=ax.transAxes, fontsize=13, fontweight='bold', va='top', ha='right')\n",
    "        if i_x == 0:\n",
    "            ax.legend(frameon=False, loc = 'lower right',fontsize=9)\n",
    "\n",
    "\n",
    "        \n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('../Figures/90CI_SN50500_vertical_m.jpeg', dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00571af7",
   "metadata": {},
   "source": [
    "### 2.2 Calculate the fitted equations \n",
    "\n",
    "->> describing the relationship between percent error (Y) and sample size (n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fd3529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. calculate percent_error\n",
    "\n",
    "percent_error = np.full([num_of_files,2,7],np.nan) # 行为217个实验，第一列为上限的error，第二列为下限的error，7为7个指标\n",
    "for i in range(7):\n",
    "    \n",
    "    percent_error[:,0,i] = (confidence_level_random[:,0,i]-statistic_value(new_merged_series_value)[i])/statistic_value(new_merged_series_value)[i]*100\n",
    "    percent_error[:,1,i] = (confidence_level_random[:,1,i]-statistic_value(new_merged_series_value)[i])/statistic_value(new_merged_series_value)[i]*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc5d158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_error_func_up(n, a, b):\n",
    "    return np.exp(a * np.log(n) + b)\n",
    "\n",
    "def percent_error_func_down(n, a, b):\n",
    "    return -np.exp(a * np.log(n) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae613f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "\n",
    "# 2. calculate coefficient of the fitted equation by using percent_error\n",
    "\n",
    "percent_error_coefficient = np.full([7,2,2],np.nan)  #The 1st column is coefficient a, the 2nd column is coefficient b, the first dimension is up, the second dimension is down\n",
    "errorbar_ylabel_name = ['Mean (%)', 'Standard deviation (%)','Skewness (%)','Kurtosis (%)','Weibull k parameter (%)','Weibull c parameter (%)','Energy density (%)']\n",
    "\n",
    "for i in range(7): # \n",
    "    \n",
    "    popt_up, _ = curve_fit(percent_error_func_up, x_scatter[0,:], percent_error[:,0,i])\n",
    "    percent_error_coefficient[i,0,0], percent_error_coefficient[i,1,0] = popt_up #a__up, b_up = poptup\n",
    "\n",
    "    popt_down, _ = curve_fit(percent_error_func_down, x_scatter[0,:], percent_error[:,1,i])\n",
    "    percent_error_coefficient[i,0,1], percent_error_coefficient[i,1,1] = popt_down #a__up, b_up = popt_down\n",
    "    \n",
    "    print(errorbar_ylabel_name[i])\n",
    "    print(f\"Y=exp[{round(percent_error_coefficient[i,0,0],3)}ln(n)+{round(percent_error_coefficient[i,1,0],3)}]\")\n",
    "    print(f\"Y=-exp[{round(percent_error_coefficient[i,0,1],3)}ln(n)+{round(percent_error_coefficient[i,1,1],3)}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78343e5f",
   "metadata": {},
   "source": [
    "#### 2.2.1 Plot figure 4 in the manuscript (plot the percent_error with fitted curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92ae575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters (start from 10 days to 16 years)\n",
    "\n",
    "interval_for_figure = 24*10 \n",
    "start_range_for_figure = 24*30 \n",
    "end_range_for_figure = 24*365*16  \n",
    "\n",
    "\n",
    "num_values = int((end_range_for_figure-start_range_for_figure)/interval_for_figure + 1)\n",
    "\n",
    "x_exp = []\n",
    "for i in range(num_values):\n",
    "    x_exp.append(start_range_for_figure + interval_for_figure*i) \n",
    "\n",
    "\n",
    "fitting_value2_up = pd.DataFrame()\n",
    "fitting_value2_down = pd.DataFrame()\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(8, 5))\n",
    "\n",
    "fig.delaxes(axes[1, 3])  \n",
    "\n",
    "\n",
    "ylim_value = {'Mean':[-10,10],'Std':[-10,10],'Skewness':[-60,60],'Kurtosis':[-360,360],'Shape':[-10,10],'Scale':[-10,10],'Energy density':[-26,26]}\n",
    " \n",
    "\n",
    "for i, ax in zip(range(7), axes.flatten()):\n",
    "    \n",
    "    # 第一种选择：看全部的参数\n",
    "    ax.set_ylabel(errorbar_ylabel_name[i])\n",
    "    ax.set_xlabel('Number of data')\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_ylim(ylim_value[prob_name[i]])\n",
    "    \n",
    "    ax.plot(x_exp, percent_error_func_up(x_exp, percent_error_coefficient[i,0,0], percent_error_coefficient[i,1,0]), color='orange',linewidth=2)\n",
    "    ax.plot(x_exp, percent_error_func_down(x_exp, percent_error_coefficient[i,0,1], percent_error_coefficient[i,1,1]), color='orange',linewidth=2)\n",
    "\n",
    "    ax.scatter(x_scatter[0,:], percent_error[:,0,i], label='Original Data',marker='o', facecolors='none', edgecolors='k', linewidths=0.5)\n",
    "    ax.scatter(x_scatter[0,:], percent_error[:,1,i], label='Original Data',marker='^', facecolors='none', edgecolors='k', linewidths=0.5)\n",
    "\n",
    "    fitting_value2_up[errorbar_ylabel_name[i]] = percent_error_func_up(x_exp, percent_error_coefficient[i,0,0], percent_error_coefficient[i,1,0])\n",
    "    fitting_value2_down[errorbar_ylabel_name[i]] = percent_error_func_down(x_exp, percent_error_coefficient[i,0,1], percent_error_coefficient[i,1,1])    \n",
    "    \n",
    "    ax.grid(True)\n",
    "    ax.minorticks_on()\n",
    "\n",
    "           \n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('../Figures/hourly_exp_percent_error_all_parameters_16years_SN50500.jpeg', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efda1285",
   "metadata": {},
   "source": [
    "#### 2.2.2 Plot figure 5 in the manuscript (plot the percent_error with fitted curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c61a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters (start from 1 day to 30 days)\n",
    "\n",
    "interval_for_figure = 24*1 \n",
    "start_range_for_figure = 24*1 \n",
    "end_range_for_figure = 24*30  \n",
    "\n",
    "num_values = int((end_range_for_figure-start_range_for_figure)/interval_for_figure + 1)\n",
    "\n",
    "x_exp = []\n",
    "for i in range(num_values):\n",
    "    x_exp.append(start_range_for_figure+interval_for_figure*i) \n",
    "\n",
    "  \n",
    "\n",
    "fitting_value1_up = pd.DataFrame()\n",
    "fitting_value1_down = pd.DataFrame()\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(8, 5))\n",
    "fig.delaxes(axes[1, 3])  \n",
    "\n",
    "ylim_value = {'Mean':[-40,40],'Std':[-40,40],'Skewness':[-300,300],'Kurtosis':[-2000,2000],'Shape':[-40,40],'Scale':[-40,40],'Energy density':[-100,100]}\n",
    "errorbar_ylabel_name = ['Mean (%)', 'Standard deviation (%)','Skewness (%)','Kurtosis (%)','Weibull k parameter (%)','Weibull c parameter (%)','Energy density (%)']\n",
    "\n",
    "for i, ax in zip(range(7), axes.flatten()):\n",
    "    \n",
    "\n",
    "    ax.set_ylabel(errorbar_ylabel_name[i])\n",
    "    ax.set_xlabel('Number of data')\n",
    "    ax.set_ylim(ylim_value[prob_name[i]])\n",
    "\n",
    "    \n",
    "    ax.plot(x_exp, percent_error_func_up(x_exp, percent_error_coefficient[i,0,0], percent_error_coefficient[i,1,0]), color='orange',linewidth=2)\n",
    "    ax.plot(x_exp, percent_error_func_down(x_exp, percent_error_coefficient[i,0,1], percent_error_coefficient[i,1,1]), color='orange',linewidth=2)\n",
    "\n",
    "    \n",
    "    fitting_value1_up[errorbar_ylabel_name[i]] = percent_error_func_up(x_exp, percent_error_coefficient[i,0,0], percent_error_coefficient[i,1,0])\n",
    "    fitting_value1_down[errorbar_ylabel_name[i]] = percent_error_func_down(x_exp, percent_error_coefficient[i,0,1], percent_error_coefficient[i,1,1])\n",
    "\n",
    "    \n",
    "    ax.grid(True)\n",
    "    ax.minorticks_on()\n",
    "       \n",
    "           \n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('../Figures/hourly_exp_percent_error_all_parameters_30days_SN50500.jpeg', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d48b79",
   "metadata": {},
   "source": [
    "#### 2.2.3 Plot figure 7 in the manuscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6a1ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '031710-99999_parameters_under_sequential_experiment'\n",
    "table_continuous = get_exp_data(output_dir,'continous')\n",
    "confidence_level_continuous = get_confidence_level(table_continuous)\n",
    "maindataset_parameters = statistic_value(df_ip)\n",
    "statisc_totalseries = get_statisc_of_series(df_ip)\n",
    "\n",
    "yrs_in_total = 12\n",
    "\n",
    "\n",
    "HOURS_PER_YEAR  = 8760.0\n",
    "HOURS_PER_MONTH = 365.25/12*24.0  # ≈ 730.5\n",
    "\n",
    "fig, axes = plt.subplots(nrows=7, ncols=1, figsize=(3, 16), sharex=False)\n",
    "\n",
    "months_list = []\n",
    "for month in range(1, 12 * yrs_in_total + 1):\n",
    "    months_list.append(month)\n",
    "\n",
    "for i, ax in zip(range(7), axes.flatten()):\n",
    "    if ax.get_visible():\n",
    "\n",
    "        \n",
    "        x_hours = np.asarray(months_list) * HOURS_PER_MONTH\n",
    "\n",
    "        \n",
    "        ax.set_xscale('log')\n",
    "\n",
    "        ymin_val_5 = min(statisc_totalseries[i,3], statisc_totalseries[i,4])\n",
    "        ymax_val_5 = max(statisc_totalseries[i,3], statisc_totalseries[i,4])\n",
    "        \n",
    "        \n",
    "        ymin_val_10 = min(statisc_totalseries[i,1], statisc_totalseries[i,2])\n",
    "        ymax_val_10 = max(statisc_totalseries[i,1], statisc_totalseries[i,2])\n",
    "        \n",
    "        \n",
    "        \n",
    "        ymin_val_2 = min(statisc_totalseries[i,5], statisc_totalseries[i,6])\n",
    "        ymax_val_2 = max(statisc_totalseries[i,5], statisc_totalseries[i,6])\n",
    "        \n",
    "\n",
    "\n",
    "        hspan1 = ax.axhspan(ymin=ymin_val_5, ymax=ymax_val_5, color='gray', alpha=0.3, label='±5% uncertainty range')\n",
    "        hspan2 = ax.axhspan(ymin=ymin_val_10, ymax=ymax_val_10, color='gray', alpha=0.2, label='±10% uncertainty range')\n",
    "        hspan3 = ax.axhspan(ymin=ymin_val_2, ymax=ymax_val_2, color='gray', alpha=0.5, label='±2% uncertainty range')\n",
    "\n",
    "        line1, = ax.plot(x_hours, confidence_level_continuous[:,0,i], color='#3A3042', linewidth=1.5, label='90% CI (continuous)')\n",
    "        ax.plot(  x_hours, confidence_level_continuous[:,1,i], color='#3A3042', linewidth=1.5)\n",
    "        line2, = ax.plot(x_hours, confidence_level_random[:,0,i],     color='orange', linewidth=1.5, label='90% CI (random)')\n",
    "        ax.plot(  x_hours, confidence_level_random[:,1,i],            color='orange', linewidth=1.5)\n",
    "\n",
    "        # Add main dataset parameters as red star\n",
    "        scatter = ax.scatter(16*HOURS_PER_YEAR, maindataset_parameters[i], marker='*', color='r', s=8, label='Main dataset parameters')\n",
    "\n",
    "        ax.minorticks_on()\n",
    "        ax.set_ylabel(ylabel_name[i], fontsize=12)\n",
    "\n",
    "        if i == 0:\n",
    "            ax.set_title('031710-99999')\n",
    "\n",
    "        if i == 6:\n",
    "            ax.set_xlabel('Number of data')\n",
    "\n",
    "\n",
    "        to_years    = lambda h: h / HOURS_PER_YEAR\n",
    "        from_years  = lambda y: y * HOURS_PER_YEAR\n",
    "        secax = ax.secondary_xaxis('top', functions=(to_years, from_years))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../Figures/90CI_differences_031710_vertical_updated.jpeg', dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d30633",
   "metadata": {},
   "source": [
    "## 2.3 Comparison with ERA5 data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd62dc9b",
   "metadata": {},
   "source": [
    "### 2.3.1 read the data from the sampling experiments for ERA5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c7c690",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_10m = \"../Data/ERA5_selectedyr_10m_SN50500.xlsx\"\n",
    "file_path_10m.replace(\"\\\\\",\"/\")\n",
    "\n",
    "df_10m = pd.read_excel(file_path_10m)\n",
    "\n",
    "statisc_totalseries_era5_10m = get_statisc_of_series(df_10m['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a728f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "interval = 24*10 \n",
    "start_range = 24*30  \n",
    "end_range = 24*365*6  \n",
    "\n",
    "num_of_files = int((end_range-start_range)/interval + 1)\n",
    "\n",
    "\n",
    "folder_path = \"../Data/ERA5_density_random/10m/SN50500\"\n",
    "\n",
    "table_era5_10m, _ = get_exp_data(folder_path,'result_iteration', num_of_files, start_range, interval)\n",
    "confidence_level_era5_10m = get_confidence_level(table_era5_10m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6a43cf",
   "metadata": {},
   "source": [
    "### 2.3.2 Plot figure 6 in the manuscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f31749",
   "metadata": {},
   "outputs": [],
   "source": [
    "ylim_value = {'Mean':[3.,5.4],'Std':[2.2,3.],'Skewness':[0.,2.],'Kurtosis':[-3.8,8],'Shape':[1.2,2.3],'Scale':[3.6,6],'Energy density':[60,160]}\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(2.6, 7))\n",
    "\n",
    "\n",
    "\n",
    "for i, ax in zip(range(3), axes.flatten()):\n",
    "    \n",
    "    \n",
    "    if ax.get_visible():  \n",
    "        \n",
    "\n",
    "        if i == 0:\n",
    "            i_x = 0\n",
    "            ax.set_title('SN50500')\n",
    "            ax.set_ylabel(ylabel_name[i_x], fontsize=12)\n",
    "            \n",
    "        elif i==1:\n",
    "            i_x = 5\n",
    "            ax.set_ylabel(ylabel_name[i_x], fontsize=12)\n",
    "            \n",
    "        elif i==2:\n",
    "            i_x = 6\n",
    "            ax.set_ylabel('Power density $(W\\ m^{-2})$', fontsize=12)\n",
    "            ax.set_xlabel('Number of data')\n",
    "\n",
    "        ax.set_xscale('log') \n",
    "\n",
    "\n",
    "        ax.axhspan(ymin=statisc_totalseries[i_x,4], ymax=statisc_totalseries[i_x,3], color='grey', alpha=0.3) # 5%\n",
    "        ax.axhspan(ymin=statisc_totalseries[i_x,6], ymax=statisc_totalseries[i_x,5], color='grey', alpha=0.8) # 2%\n",
    "\n",
    "        ax.axhspan(ymin=statisc_totalseries_era5_10m[i_x,4], ymax=statisc_totalseries_era5_10m[i_x,3], color=\"#EFF3FF\", zorder=0) # 5%\n",
    "        ax.axhspan(ymin=statisc_totalseries_era5_10m[i_x,6], ymax=statisc_totalseries_era5_10m[i_x,5], color=\"#B0C4DE\", zorder=0) # 2%\n",
    "    \n",
    "        ax.set_ylim(ylim_value[prob_name[i_x]])\n",
    "        for j in range(num_of_files+1):\n",
    "\n",
    "            if j == num_of_files:\n",
    "                ax.scatter(len(new_merged_series_value), statistic_value(df_10m['value'])[i_x], marker='*', color='r',s=8) # 显示这个数据集的值\n",
    "                ax.scatter(len(new_merged_series_value), statistic_value(new_merged_series_value)[i_x], marker='*', color='#373737', s=8)\n",
    "            else:\n",
    "                \n",
    "                ax.scatter(x_scatter[:,j], table_era5_10m[:,j,i_x], marker='*', color='k',s=1, alpha=0.2, zorder=1)\n",
    "\n",
    "\n",
    "\n",
    "        # 展示90%的范围\n",
    "        ax.plot(x_scatter[0,:], confidence_level_era5_10m[:,0,i_x], color=\"#E69F00\",label='ERA5 10m', linewidth=2, zorder=3)\n",
    "        ax.plot(x_scatter[0,:], confidence_level_era5_10m[:,1,i_x], color=\"#E69F00\", linewidth=2, zorder=3)\n",
    "\n",
    "        ax.plot(x_scatter[0,:], confidence_level_random[:,0,i_x], color='#373737', alpha=0.8 ,label='In-situ', linewidth=2, zorder=4)\n",
    "        ax.plot(x_scatter[0,:], confidence_level_random[:,1,i_x], color='#373737', alpha=0.8, linewidth=2, zorder=4)\n",
    "\n",
    "        \n",
    "        ax.minorticks_on()\n",
    "        \n",
    "        if i == 0:\n",
    "            ax.legend(frameon=False, loc = 'center right',fontsize=9)\n",
    "\n",
    "\n",
    "        \n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('../Figures/ERA5_10m_distribution_parameters_SN50500_vertical_m.jpeg', dpi=300)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
